\documentclass[12pt,a4paper,twoside]{article}
\usepackage{bookmark}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[margin=25mm]{geometry}
% \usepackage[backend=biber]{biblatex}
% \addbibresource{stankeviciute-proposal.bib}

\begin{document}

\begin{center}
\Large
Computer Science Tripos -- Part II -- Project Proposal\\[4mm]
\LARGE
A graph convolutional network for [...] disease classification\\[4mm]

\large
Kamilė Stankevičiūtė (\texttt{ks830}), Gonville \& Caius College

\today % October 2019
\end{center}

\vspace{5mm}
\textbf{Project Originator:} Tiago Azevedo

\textbf{Project Supervisor:} Tiago Azevedo, Prof Pietro Liò

\textbf{Directors of Studies:} Dr T.~M.~Jones, Prof P.~Robinson, Dr G.~Titmus

\textbf{Project Overseers:} 

% Main document

\section*{Introduction}
% The problem to be addressed.

\subsection*{Background}

\textit{Description of the subject of the graph convolutional network, e.g. Alzheimer's, Autism Spectrum disease,...}

Neural networks provide the opportunity to capture the similarities between patients and trends which might help physicians to understand the mechanisms of the disease and in turn find more effective treatments.

\textit{Give more information on graph neural networks + spectral convolutions}

\textit{Application of the GCN to patient population context.} One way of modelling the population with some patients having [...] is a graph, with nodes representing individuals and their features (including the diagnosis), and edges corresponding to associations between individuals according to some heuristic or a formally defined similarity metric. Additionally, the graph structure is helpful when incorporating multiple modalities of data (e.g. imaging and non-imaging).

One reason why such graph representation is considered to be useful is that it makes use of both the individual patient data (node feature vectors) and the trends in the population or pairwise similarities between different patients (graph edges), inferring the patient's diagnosis from their neighbourhood. 

% This project will explore such graph convolutional networks (GCNs) as well as alternative geometric deep learning approaches to classify the two forms of Mild Cognitive Impairment (MCI): \textit{stable} and \textit{progressive} (progressing to AD).

\textit{Description of the dataset—but not sure which section it should appear in, right now it's in Introduction, Starting point and Resources.}

\subsection*{Project Description}
\textbf{Data, Models, Evaluation}

\begin{itemize}
  \item Applying GCN to patient populations
  \item Diagnosis classification task (e.g. MCI to AD progression—stable vs progressive MCI—using ADNI dataset, or ASD diagnosis using the ABIDE dataset)
  \item Using PyTorch rather than TensorFlow to make use of the broader range of APIs supporting graph data in PyG—\textit{making the final library more accessible and extensible}
  \item \textit{Training and test data}
\end{itemize}

\section*{Starting point}

% Describe existing state of the art, previous work in this area,
%   libraries and databases to be used. Describe the state of any
%   existing codebase that is to be built on.

This project will be based on a state-of-the-art graph convolutional network (GCN) as described in papers by Parisot et al. \cite{parisot2017spectral, parisot2018disease} In these papers, the GCN (adapted from Kipf \& Welling \cite{kipf2017semi}) was used for two classification tasks: 1) whether a patient is  healthy or suffering from Autism Spectrum Disorder) and 2) whether a patient having Mild Cognitive Impairment has a stable form or the one progressing to Alzheimer's disease. The GCN has achieved the accuracies of 70.4\% and 80.0\% for the two tasks respectively. 

The source code of this paper (written in TensorFlow) is publicly available at \url{http://github.com/parisots/population-gcn}. It is fully implemented for the first classification task but not the second. This will be used as a basis for replication of the results on PyTorch and extending the model additional improvements. PyTorch has been chosen for its libraries specialised for machine learning on structured graph data—particularly the \texttt{torch\_geometric} (PyG) package—which will make iteration and extensions to the model more flexible as well as improve its performance and simplify the APIs.

This project will use the ADNI dataset (same as in \cite{parisot2017spectral,parisot2018disease}). The database is available at \url{http://adni.loni.usc.edu}. [to change depending on the actual dataset chosen]

\textbf{Relevant Computer Science Tripos courses:}
\begin{itemize}
  \item Machine Learning and Real-World Data (IA 75\%)
  \item Scientific Computing (IA 75\%)
  \item Foundations of Data Science (IB 75\%)
  \item Artificial Intelligence (IB 75\%)
  \item Bioinformatics (II 75\%)
  \item Data Science: Principles and Practice (II 75\%)
  \item Machine Learning and Bayesian Inference (II 75\%)
  \item \textit{any software engineering courses that told me that I should do the tests}
\end{itemize}

\textbf{Relevant previous knowledge about the topic:}
\begin{itemize}
  \item 5-course Deep Learning specialisation by deeplearning.ai on Coursera (with basics of TensorFlow and Python APIs).
  \item Google's Machine Learning Crash Course along with follow-up courses on data preparation, problem framing, testing and debugging (basics of TensorFlow and Python APIs).
\end{itemize}

\section*{Resources required}

For the most part of this project I will be using my personal MacBook Pro (2019, quad-core 1.4 GHz Intel Core i5 processor, 8 GB LPDDR3 RAM) running macOS Catalina. Training the model will most likely require the use of GPUs provided by the Computational Biology Group (as confirmed by Prof Pietro Liò).

The following measures will be taken to store the work and reduce the likelihood of any loss of data: 
\begin{itemize}
  \item Saving the source code and \LaTeX\ source of the project on my machine, private GitHub repositories, and Google Drive (possibly MCS).
  \item Regularly backing up the contents of my laptop on an external HDD.
\end{itemize}

\subsection*{Dataset}
\textit{Change to another dataset if appropriate.}

The data required for training (ADNI database) is available on \url{http://adni.loni.usc.edu}, for which I requested and was granted access.

\section*{Work to be done}
\label{section:work}

% Describe the technical work.
List of explicit \textbf{deliverables}.
\begin{itemize}
  \item Preprocess the data (depending on the dataset). For example, ADNI would require fetching the data from the database and preprocessing the brain volumes into $C=138$ longitudinal scans ("large scale segmentation analysis (into 138 anatomical structures using MALP-EM (Ledig et al., 2015))") as in the Parisot paper. Fetching for the other datasets. \textit{Deliverable}: data processing pipeline.
  \item Connecting the datapoints into the graph based on similarity criteria as defined in the paper (if the dataset is applicable). \textit{Deliverable}: processing the fetched datapoints to connect them into the graph based on pairwise similarity and the definition of the similarity metric/threshold.
  \item Implement of the GCN that reproduces the results of the paper (depending on the dataset). \textit{Deliverable}: Kipf's GCN (\texttt{GCNConv} in PyG) that can be trained on the graph constructed into the previous point. \textit{Deliverable}: The implementation of the graph neural network with the hyperparameters as listed in the paper (if applicable for the dataset) that reproduces the results.
  \item An alternative graph neural network for performance comparison. One interesting choice could be a Graph Attention Network (GAT), different from \texttt{GCNConv} due to the use of weighted edges. Note that for this to be used a custom similarity metric should be implemented that gives continuous (rather than binary) edge weights. \textit{Deliverable}: a graph neural network that uses a different approach for convolutions, such as GAT.
  \item \textit{Testing framework.} This is a good software engineering practice that makes it more likely that the code does not contain significant logic errors or other bugs. An exact testing framework could be developed for the preprocessing part, perhaps some \textit{small} edge cases in the graph neural network could be verified using random seeds. \textit{Deliverable}: Testing framework.
  \item \textit{Evaluation framework.} One interesting evaluation criterion that is not listed in the Parisot papers which should be implemented in any case (accuracy, AUC), is the measure of \textit{robustness}, which could be defined as the \textit{rate at which the accuracy drops as more information is removed from the nodes}. It is a good way to see how much performance is dependent on the \textit{connections} rather than the \textit{individual patient data}—how much can we infer from the similar neighbours alone without knowing anything about the patient in question. \textit{Deliverable}: evaluation based on the metrics as described in Parisot papers—accuracy, AUC—for direct comparison with the baseline methods. \textit{Deliverable}: an API that takes in the algorithm and the GCN, reports the accuracy measures at each level of removed input data. Should be able to return the actual graph of this accuracy dropping.
  \item (?) Statistical processing (for evaluation).
  \item Writing up the dissertation. \textit{Deliverable}: obvious.
\end{itemize}


\textit{Rough list of things to do (based on ADNI, change to another dataset if appropriate).}
\begin{itemize}
  \item Read Kipf and Welling's GCN paper \cite{kipf2017semi} and blog post. Read about spectral graph theory because that seems to be the basis of graph convolutions [seems difficult to grasp—also revise/study more linear algebra]
  \item Get familiar with \texttt{torch\_geometric} (PyG), TensorFlow
  \item Work on fetching and pre-processing the data (e.g. fetch ADNI dataset).
  \item Get the working implementation for TensorFlow GCN for ADNI dataset, reproduce the results.
  \item Measure robustness of the original model in TensorFlow GCN
  \item Write the ADNI implementation using PyG, measure accuracy (aim for at least 80.0\%)
  \item Find alternative methods in PyG that might work better and improve accuracy
  \item Measure robustness of the new model—if robustness decreases the new model from previous step might be overfitting
  \item Incorporate more data from ADNI, if there is too much data consider using autoencoders and other dimensionality reduction techniques.
\end{itemize}

\section*{Success criteria}

% Describe what you expect to be able to demonstrate at the
% end of the project and how you are going to evaluate your achievement.

% \begin{itemize}
%   \item Reimplement the model in PyTorch (\texttt{pytorch\_geometric}) package with the base accuracy of at least 80.0\% as in \cite{parisot2018disease}
%   \item \textsc{todo}
% \end{itemize}


The above list suggests that reproducing the result from \cite{parisot2018disease} (80\% accuracy and probably similar AUC) should be the baseline. Then extensions to the model should increase those results.

Another metric that could be defined is \textit{robustness to missing or incorrect data}, revealing how important is the neighbourhood information in accurately predicting the diagnosis compared to the node features only.

\section*{Possible extensions}
% Potential further envisaged evaluation metrics or extensions.
\begin{itemize}
  \item Implement spectral filter computation with \textit{Cayley polynomials} instead of using Chebyshev polynomials. Cayley polynomials have been introduced in a paper by Levie et al. \cite{levie2017cayleynets} and were mentioned in \cite{parisot2018disease} as a possible improvement.
  \item Implement a \textit{custom similarity metric}. Currently the metrics used are defined arbitrarily by the authors based on very few features. Learning a different similarity metric based on more features (such as genetic sequencing—SNPs in APOE gene, TOMM40 gene, whole genome sequencing, more types of MRI or PET images in ADNI) could possibly result in a better performance of the classifier.
\end{itemize}


\section*{Timetable}
\label{section:timetable}

% A work plan of perhaps ten or so two-week work-packages,
% as well as milestones to be achieved along the way. Provide a
% target date for each milestone.

Planned starting date is 01/10/2019.

% 10/10/2019—16/10/2019
\subsection*{Michaelmas weeks 0-1}
\textit{Preparation.}

\textbf{Milestones.} Submit Phase 1 report by 14/10/2019. Submit draft proposal by 18/10/2019.

% (Michaelmas weeks 2--4), 17/10/2019—06/11/2019
\subsection*{Michaelmas weeks 2-4}
\textit{Preparation.}

\textbf{Milestones.} Submit final project proposal by 25/10/2019.

% (Michaelmas weeks 5--6), 07/11/2019—20/11/2019
\subsection*{Michaelmas weeks 5-6}
\textit{Core work.}

% (Michaelmas weeks 7--8), 21/11/2019—04/12/2019
\subsection*{Michaelmas weeks 7-8}
\textit{Core work.}

\subsection*{Michaelmas vacation}
\textit{Core work. Some write-up. Progress report and presentation.}

\textbf{Milestones.} Complete the implementation of the main part of the project.

% (Lent weeks 0--2), 16/01/2020—29/01/2020
\subsection*{Lent weeks 0-2}
\textit{Extensions.}
 
\textbf{Milestones.} Submit progress report by 31/01/2020.

% (Lent weeks 3--5), 30/01/2020—19/02/2020
\subsection*{Lent weeks 3-5} 
\textit{Extensions, start evaluation.}

% (Lent weeks 6--8), 20/02/2020—11/03/2020
\subsection*{Lent weeks 6-8}
\textit{Extensions, evaluation, write-up.}

\subsection*{Easter vacation}
\textit{Write-up.}

\textbf{Milestones.} Send out the complete draft for review by 27/03/2020. Early dissertation submission by 20/04/2020.

% 24/04/2020—06/05/2020
\subsection*{Easter weeks 0-2}
 Time reserved for any unexpected issues.

% \medskip 
% \printbibliography
\bibliographystyle{unsrt}
\bibliography{stankeviciute_project_proposal}

\end{document}
