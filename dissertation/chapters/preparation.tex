\chapter{Preparation}
% Principally, this chapter should describe the work which was undertaken before code was written, hardware built or theories worked on. It should show how the project proposal was further refined and clarified, so that the Implementation stage could go smoothly rather than by trial and error.
% Throughout this chapter and indeed the whole dissertation, it is essential to demonstrate that a proper professional approach was employed.
% The nature of this chapter will vary greatly from one dissertation to another but, underlining the professional approach, this chapter will very likely include a section headed “Requirements Analysis” and incorporate other references to software engineering techniques.
% The chapter will cite any new programming languages and systems which had to be learnt and will mention complicated theories or algorithms which required understanding.
% It is essential to declare the Starting Point (see Section 7). This states any existing codebase or materials that your project builds on. The text here can commonly be identical to the text in your proposal, but it may enlarge on it or report variations. For instance, the true starting point may have turned out to be different from that declared in the proposal and such discrepancies must be explained.

This section presents the mathematical definition of the population graphs, the neuroimaging data those graphs will incorporate, and the neural network architectures that will be used to train the population graphs.

% In addition it presents the requirements to the implementation of this project and the software engineering principles that have been used to ensure 

\section{Brain age estimation}
To estimate the brain age gap the difference between the chronological and the brain age must be known. The common technique is to develop a machine learning method to estimate the chronological age from the brain imaging features. The estimated age is then considered to be the brain age although it has been trained to estimate the chronological age and the actual value of the brain age is never known. The reason is why it works... \cite{niu2019improved}

We represent the brain age $y_b$ as the sum of the known chronological age $y_c$ and the unknown brain age gap $\varepsilon_g$:

\begin{equation}
    y_b = y_c + \varepsilon_g.
\end{equation}

On the other hand, the machine learning model estimates the brain age $y_b$ from some function of the brain imaging features $X$ and a prediction error $\varepsilon_e$:

\begin{equation}
    y_b = f(X) + \varepsilon_e.    
\end{equation}

Expressing $y_c$ as $y_b - \varepsilon_g$ and substituting the previous result, we get the estimate of chronological age as

\begin{equation}
    y_c = f(X) + \varepsilon_g - \varepsilon_e,
\end{equation}

Since the machine learning models estimate chronological age as a function of brain imaging features (with some estimation error $\varepsilon_e'$)

\begin{equation}
    y_c = f(X) + \varepsilon_e',
\end{equation}

the error of the proposed machine learning model $\varepsilon_e'$ will contain in itself both the brain age gap $\varepsilon_g$ (which we are interested in) and the brain age prediction error $\varepsilon_e$.

It is out of scope of this dissertation to prove that the brain age gap component $\varepsilon_g$ is larger than the error term; however, the keen reader is referred to Niu et al. \cite{niu2019improved} where this is verified through correlation of the brain age gaps to the cognitive behaviour scores.

\section{Neuroimaging dataset}

\subsection{United Kingdom Biobank}

The United Kingdom Biobank (UK Biobank) \cite{sudlow2015uk} is a continuous, large, population-wide study of over 500,000 participants containing a wide range of phenotypic and genetic data that is used by the researchers to analyse the risk factors and development of various health conditions. 

Of particular relevance to this dissertation are the UK Biobank participants with neuroimaging data records that have been denoised, motion-corrected and otherwise processed for further analysis, a total of 17,550 participants. The data has been initially preprocessed with the standard UK Biobank pipelines,\footnote{\url{https://biobank.ctsu.ox.ac.uk/crystal/crystal/docs/brain_mri.pdf}} and further denoised, parcellated and kindly provided by Dr Richard Bethlehem of the Department of Psychiatry. The Department of Psychiatry pipelines have been co-authored with Dr Rafael Romero-Garcia and Dr Lisa Ronan.\footnote{\url{https://github.com/ucam-department-of-psychiatry/UKB}} The details on the neuroimaging dataset are described below.

\subsection{Parcellation}
A \textit{parcellation} or \textit{atlas} refers to the way the brain is split into meaningful regions for further analysis. Whether two voxels of a brain belong to the same parcel may depend on their proximity, empirical evidence of that the voxels are responsible for the same function and so forth, and can be used to compare the locations in two different brains. When the brain is imaged there is a choice whether to warp the image of the brain to the fixed atlas or whether to warp the atlas to match the variable brain images. The former makes it easier to process a dataset of many images and find the matching regions of two brains faster, but the latter remains more faithful to the unique structure of the individual patient's brain. 

Both functional and structural datasets use one of the most common parcellations by Glasser et al. \cite{glasser2016multi}, which divides the brain into 360 cortical regions and 16 subcortical regions. 

\subsection{Structural data}
Structural brain imaging data encapsulates brain features related to its structure, such as cortical thickness, white 

Combined T1-weighted and T2-weighted FLAIR images which emphasise different aspects of the MRI scan and therefore might help to extract different structural features. For the dataset used in this dissertation, the combination of the two types of images to derive the structural features using the HCP Freesurfer pipeline.\footnote{\url{https://www.ncbi.nlm.nih.gov/pubmed/23668970}} 

\subsection{Euler indices}
Euler index\footnote{\url{https://www.ncbi.nlm.nih.gov/pubmed/29278774}} is a quality control measure and corresponds to the number of times the Freesurfer brain reconstruction software failed to connect two slices of an MRI image. The higher the Euler index, the worse is the quality of the scan. Euler indices might be used to remove the subjects with low-quality scans to avoid them affecting the analysis \cite{kaufmann2019}. Otherwise they can be used as a covariate in a machine learning model (as brain similarity metric or a node feature) to correct for any bias in prediction that might be related to scan quality.

\subsection{Functional data}
The resting state functional MRI (rs-fMRI) is the representation of the brain activity over time. In MRI scanner this is measured by the changes in blood oxygenation as neural activity regulates the oxygen demand, resulting in blood oxygenation level dependent (BOLD) time-series measured at each voxel of the brain.

\textit{figure of BOLD timeseries}

We are interested in estimating which parts of the brain are connected to each other, which we do by making use of the assumption that parts of the brain that have related function would also have similar activity patterns. As a consequence, we would expect higher correlation of the corresponding BOLD time-series. For time-series $T_1$ and $T_2$, \textit{Pearson's correlation} $r$ is computed as

\begin{equation}
    r(T_1, T_2) = \frac{\mathrm{cov}(T_1, T_2)}{\sigma_{T_1} \sigma_{T_2}}
\end{equation}

where $\mathrm{cov}(\cdot, \cdot)$ denotes covariance and $\sigma$ stands for standard deviation.

It is common to derive the \textit{functional connectivity matrix} that stores the pairwise correlations between the different voxels as the overall representation of functional brain connectivity. For time-series $T_1, \dots, T_N$,

\begin{equation}
    \mathrm{fcm}(T_1, \dots, T_N) = \begin{bmatrix}
        r(T_1, T_1) & \cdots & r(T_1, T_N) \\
        \vdots & \ddots & \vdots \\
        r(T_N, T_1) & \cdots & r(T_N, T_N)
    \end{bmatrix},
\end{equation}

\textit{of which (due to symmetry and the non-informative diagonal) only the flattened lower triangle is usually used for the machine learning implementations.}

\subsection{Phenotype data}
For the population graph construction, the neuroimaging features are associated with the individual subjects (nodes). The similarity metric is defined by phenotype data, which is all important but not directly neuroimaging-related data. Some examples of phenotype data that is related to the brain include the patient's sex, mental health, other potential health issues, full-time education, bipolar disorder status etc. Indeed the metric that is being predicted is correlated with the brain tissue age indicative of various neurological and neurodegenerative diseases.

\textit{include a table of the actual phenotypes used when the results are ready}

\section{Population graphs}
We connect the multi-modal MRI imaging (structural and/or functional), quality control and phenotype data of the set of patients $S$ into a sparse \textit{population graph} $\mathcal{G} = \{\mathcal{V}, \mathcal{E}\}$, where $\mathcal{V}$ is the set of graph vertices (with one vertex uniquely representing one patient), and $\mathcal{E}$ is the set of edges (representing the \textit{similarity} of patients).

Each vertex $v \in \mathcal{V}$ is the vector containing the individual subject's neuroimaging data, whether structural, functional, or both. The edge $(v, w) \in \mathcal{E}$ connects patients $v$ and $w$ based on phenotypic similarity that is defined by some \textit{similarity metric} as described below.

\subsection{Similarity metrics}


\subsection{Population graph training}
Train/validation/test split, cross-validation, patient selection and exclusion from results, stratification, graph representation (edge lists, node features, edge features,...)

\section{Graph convolutional networks}

\section{Graph attention networks}

\section{Requirements analysis}

Tasks to be implemented (according to proposal: work to be done, success criteria, possible extensions), their relative importance (priority) and difficulty. Provide the order in which the tasks should be carried out to show good planning skills and account for the changes in proposal where the preprocessing pipeline turned out to be more important than the neural network implementation.

\section{Software engineering practice}
Implementing a flexible preprocessing pipeline which could be customised in the future for a variety of machine learning tasks even outside graph neural networks (a package).

Modular structure encapsulating specific task and having well defined documentations of the others.

Description of software engineering techniques: planning out and executing the project based on requirements analysis, setting tasks, and smoothly meeting the success criteria.

Code reuse (of open source well tested libraries), follow documentation and follow the PEP-8 style guide (or whatever PyCharm encourages).

Incremental development.

Modular structure: e.g. data processing, graph construction, graph neural network modules, robustness evaluation framework. Figure out where validation and cross validation sections should be (while training, separately etc.)

Diagram of the pipelines and module interaction (like in google design docs)


\section{Choice of tools}
PyTorch, PyTorch geometric extension, graph spectral filters/convolutions, message passing, timeseries preprocessing into correlation matrices, IDEs, backup strategies

\section{Starting point}
\begin{itemize}
    \item dataset, preprocessed by Dr Richard A.I. Bethlehem
    \item PyTorch, PyTorch geometric implementing GCN and GAT APIs and the graph API
    \item no previous experience with graph neural networks or the mathematics behind it
    \item no previous experience with PyTorch; limited experience with machine learning frameworks (basics of TensorFlow), no experience with neuroimaging data
\end{itemize}
