\chapter{Evaluation}
\label{chapter:evaluation}
% This is where Assessors will be looking for signs of success and for evidence of thorough and systematic evaluation as discussed in Section 8.3. Sample output, tables of timings and photographs of workstation screens, oscilloscope traces or circuit boards may be included. A graph that does not indicate confidence intervals will generally leave a professional scientist with a negative impression.
% As with code, voluminous examples of sample output are usually best left to appendices or omitted altogether.
% There are some obvious questions which this chapter will address. How many of the original goals were achieved? Were they proved to have been achieved? Did the program, hardware, or theory really work?
% Assessors are well aware that large programs will very likely include some residual bugs. It should always be possible to demonstrate that a program works in simple cases and it is instructive to demonstrate how close it is to working in a really ambitious case.

% ~2,000 words

\section{Model ranking and selection}
\label{section:model-ranking}
Following the hyperparameter tuning process described in Section~\ref{section:training-procedure}, the models were selected according to the following procedure (applied separately to the GCN and GAT model families):
\begin{enumerate}
    \item First, the models were ranked by ascending average MSE loss. The model with the lowest average MSE was chosen as the reference model.
    \item All models whose 1 standard deviation interval from their MSE did not overlap with the 1 standard deviation interval of the reference model MSE were excluded from ranking.
\end{enumerate}

The cross-validation performance of the best-scoring models selected by the above procedure is shown in Figure~\ref{figure:gat-gcn-rank}. The hyperparameters for each of the short-listed models are listed in Appendix~?.

% \begin{figure}[]
%     \centering
%     \includegraphics[width=\textwidth]{gcn_model_selection.pdf}
%     \caption{Highest scoring population graph and GCN model parameter combinations.}\label{figure:gcn-rank}
% \end{figure}

% \begin{figure}[]
%     \centering
%     \includegraphics[width=\textwidth]{gat_model_selection.pdf}
%     \caption{Highest scoring population graph and GAT model parameter combinations.}\label{figure:gat-rank}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{model_selection.pdf}
    \caption{Highest scoring population graph and GNN model parameter combinations.}\label{figure:gat-gcn-rank}
\end{figure}

Although both best-ranked (reference) models (GCN1 and GAT1) have relatively high variance, they still seem to be the most promising and therefore have been selected for further evaluation. Their population graph specification and GNN architecture hyperparameters are listed in Table~\ref{table:best-hyperparameters}.

\begin{table}[]
    \caption{Best performing population graph and GNN model parameter combinations during the model selection process.}\label{table:best-hyperparameters}
    \centering
    \small
    \begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
        \hline
    \textbf{Hyperparameter} & \textbf{GCN1} & \textbf{GAT1} \\  \hline
        Similarity feature set & \texttt{FI}, \texttt{FTE}, \texttt{ICD10}, \texttt{MEM}, \texttt{SEX} & \texttt{FI}, \texttt{ICD10}, \texttt{MEM}, \texttt{SEX} \\
        Similarity threshold & 0.9 & 0.8 \\ \hline
        Layer sizes & [1024, 512, 512, 256, 256, 1] & [2048, 1024, 512, 256, 128, 1] \\
        \# convolutional layers & 5 & 2 \\
        Dropout & $3.22 \times 10^{-1}$ & $3.14 \times 10^{-3}$ \\
        Learning rate & $6.98 \times 10^{-3}$ & $1.34 \times 10^{-2}$ \\
        Weight decay & $1.31 \times 10^{-2}$ & $6.05 \times 10^{-4}$ \\ \hline
\end{tabular}
\end{table}

\section{Evaluation metrics}
\label{section:evaluation-metrics}
The main performance metrics used for most regression problems, including brain age estimation task, are \textit{Pearson's correlation} and \textit{coefficient of determination}.

\subsubsection{Pearson's correlation}
For sets of true labels $\mathbf{y}  = [y_1 \dots y_N]$ with mean $\bar{y}$ and predicted labels $\mathbf{\hat{y}} = [\hat{y}_1 \dots \hat{y}_N]$, \textit{Pearson's correlation} is computed as

\begin{equation}
    r(\mathbf{y}, \mathbf{\hat{y}}) = \frac{\mathrm{cov}(\mathbf{y}, \mathbf{\hat{y}})}{\sigma_{\mathbf{y}} \sigma_{\mathbf{\hat{y}}}},
\end{equation}

where $\mathrm{cov}(\cdot, \cdot)$ denotes covariance and $\sigma$ stands for standard deviation. 

\subsubsection{Coefficient of determination}
The \textit{coefficient of determination} indicates how much variance in the features $\mathbf{X}$ could be explained by the model. It is computed as 
\begin{equation}
    r^2 = 1 - \frac{\sum_{i} (y_i - \bar{y})^2}{\sum_{i} (y_i - \hat{y}_i)^2}.
\end{equation}

Higher values for both metrics (with maximum 1) indicate a higher level of agreement between the true and predicted labels and therefore higher predictive power.


\section{Test set performance of selected models}
Generally in literature, after using cross-validation for model selection, the model is retrained on the entire dataset before giving a point estimate on a hold-out test set~\cite{raschka2018model}. This is because training on more data, especially when the dataset is small, allows to learn more patterns and therefore give better predictions on the unseen data. However, in this project the validation set was also used for early stopping since neural networks are especially prone to overfitting~\cite{prechelt1998automatic}. Some investigation of the hyperparameter tuning has shown that applying the stopping criteria discussed in Section~\ref{section:training-procedure} on just the training set would have still led to convergence only after the model has already overfit on the unseen validation labels. On the other hand, it is unclear how the stopping criteria should be adjusted when the training set size increases. 

Considering that the UKB dataset is large and that retraining the model with more data but without early stopping might not pay off for the loss in generalisation, all cross-validation folds were kept for test set performance estimation. Table~\ref{table:test-performance} gives the hold-out test set estimates for the metrics discussed in Section~\ref{section:evaluation-metrics}.

\begin{table}[h]
    \caption{Average test set performance of GCN1 and GAT1 models. \\ The results are within one standard deviation.}\label{table:test-performance}
    \centering
    \small
    \begin{tabular}{ccc}
        \hline
    \textbf{Model} & $r$ & $r^2$ \\  \hline
        GCN1 & $0.675 \pm 0.008$ & $0.445 \pm 0.010$ \\
        GAT1 & $0.670 \pm 0.005$ & $0.477 \pm 0.008$ \\ \hline
\end{tabular}
\end{table}


\section{Permutation testing}

\section{Population graph robustness to noisy features}
A desirable property for the real-world machine learning models is their robustness, defined as tolerance to the noise and inconsistency in data.
For example, as discussed in Section~\ref{section:implementation-robustness}, it is useful if the model can retain its performance even when the data contains particularly noisy MRI scans. Population graphs trained on graph neural networks could do this by exploiting the neighbourhoods that are hopefully less noisy than a given node. Whether they are actually capable of doing so could be tested by either adding noise to an increasing proportion of nodes or depriving the nodes from their neighbourhoods. 

For the first type of robustness evaluation, the normalised feature tensor \texttt{x} of the population graph (see Table~\ref{table:population-graph}) is perturbed with the additive white Gaussian noise of variance 0.5 (as the normalised features are between $-1$ and 1) for the increasing proportion of nodes â€“ the levels chosen were 1\%, 5\%, 10\%, 20\%, 30\% and 50\%. Following the renormalisation of the tensor, the model is retrained again on the training dataset and tested on the hold-out test set, measuring the change in performance. To make sure that any effect on the evaluation metrics is due to added noise and not the changing dataset split, the model is trained on a single dataset split while the noise is added to different subjects. For each of the GCN and GAT models, this results in five training procedures at each noise level.


\section{Population graph robustness to loss of structure}
The the purpose of the second type of robustness is to explore whether the population graph structure itself is useful in estimating the brain age for a particular node. To this end, instead of adding noise to the nodes, an increasing proportion of edges is removed from the population graph. Similarly to the previous section, the selected proportions of edges removed were 1\%, 5\%, 10\%, 20\%, 30\% and 50\%. For each of these levels the training procedure is carried out five times using a different random seed. The more edges are removed, the less neighbourhood structure the graph neural network models can exploit, having to rely on the node features only. 





% \section{Comparison against existing benchmarks}

% Compare to the Kaufmann et al.'s \textit{xgboost} approach \cite{kaufmann2019} ($r \sim 0.93$); and the other package that was cited in the same paper.
% Possibly compare to other non-graph (relatively baseline) (neural network) architectures, e.g. ElasticNet, MLP,...

